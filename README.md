# BasketLiDAR: The First LiDARâ€“Camera Multimodal Dataset for Professional Basketball MOT

**BasketLiDAR** is a multimodal dataset pairing synchronized LiDAR point clouds with RGB cameras in professional basketball scenes, designed for Multi-Object Tracking (MOT) research.  
It includes data, baseline notebooks, and standardized evaluation to facilitate reproducible experiments for LiDAR-only and cameraâ€“LiDAR fusion tracking.

> **Paper:** The project paper will be released in **November 2025**.  
> <!--â€»TODO: è«–æ–‡å…¬é–‹å¾Œã«å®ŸURLã¸ç½®ãæ›ãˆã¦ãã ã•ã„ â†’ [Details here](#)>

---

## Table of Contents
- [Data Visualization](#data-visualization)
- [Setup](#setup)
- [Datasets](#datasets)
  - [Downloads](#downloads)
  - [Folder Structure](#folder-structure)
- [LiDAR-based Tracking](#lidar-based-tracking)
- [Camera-Fusion Tracking](#camera-fusion-tracking)
- [License](#license)
- [Contact](#contact)

---

## Data Visualization
<!--â€»OPTIONAL: ã‚«ãƒ¡ãƒ©ç”»åƒã¨LiDARï¼ˆBEVã‚„ç‚¹ç¾¤ï¼‰ã®ã‚¿ã‚¤ãƒªãƒ³ã‚°æ˜ åƒãƒ»é™æ­¢ç”»ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚  
æŽ¨å¥¨ãƒ‘ã‚¹ä¾‹:
- `assets/visualizations/tiled_demo.mp4`ï¼ˆGitHub ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾ç­–ã¨ã—ã¦ `tiled_demo.gif` ã‚’ä½µç”¨ï¼‰
- `assets/visualizations/pc_overview_1.png`
- `assets/visualizations/pc_overview_2.png`

ä¾‹ï¼ˆç”»åƒã‚’è¿½åŠ ã—ãŸå ´åˆï¼‰:

![Tiled demo](assets/visualizations/tiled_demo.gif)
-->

![dataset preview](visualizations/Fig1_data.png)

---

## Setup

We recommend using a **conda** environment.

### Linux / macOS / Git Bash

```bash
# Create and activate a fresh environment
conda create -n basketlidar python=3.11 -y
conda activate basketlidar

# Install dependencies (requirements.txt is provided at the project root)
pip install -r requirements.txt
```

### Windows (PowerShell)

```powershell
conda create -n basketlidar python=3.11 -y
conda activate basketlidar

conda basketlidar update -f environment.yml
```

## Datasets

### Downloads

Access to the dataset is granted upon request for research purposes.

- Please **contact us** with your name, affiliation, and intended use.  
  â€» (#contact)

- Once approved, we will share a **Google Drive link** for download.

> Downloading and usage imply agreement with the dataset terms of use.  
> Redistribution is not allowed without permission.

```

Expected directory layout:

```text
D:.
â””â”€dataset
    â”œâ”€frames_img
    â”‚  â”œâ”€far-end
    â”‚  â”‚  â”œâ”€camera1
    â”‚  â”‚  â”‚  â”œâ”€day1_measurement3_scene_camera_frame_02524-02832
    â”‚  â”‚  â”‚  â”œâ”€day1_measurement3_scene_camera_frame_03649-04042
    â”‚  â”‚  â”‚  â”œâ”€day1_measurement3_scene_camera_frame_16273-16688
    |  |  |  ...
    â”‚  â”‚  â”œâ”€camera2
    â”‚  â”‚  â”‚  â”œâ”€day1_measurement3_scene_camera_frame_02524-02832
    â”‚  â”‚  â”‚  â”œâ”€day1_measurement3_scene_camera_frame_03649-04042
    â”‚  â”‚  â”‚  â”œâ”€day1_measurement3_scene_camera_frame_07798-08274
    |  |  |  ...
    â”‚  â”‚  â”œâ”€camera3
    â”‚  â”‚  â”‚  â”œâ”€day1_measurement3_scene_camera_frame_02524-02832
    â”‚  â”‚  â”‚  â”œâ”€day1_measurement3_scene_camera_frame_03649-04042
    â”‚  â”‚  â”‚  â”œâ”€day1_measurement3_scene_camera_frame_15075-15389
    |  |  |  ...
    â”‚  â”‚  â””â”€lidar
    â”‚  â”‚      â”œâ”€day1_measurement3_scene_camera_frame_02524-02832
    â”‚  â”‚      â”œâ”€day1_measurement3_scene_camera_frame_03649-04042
    â”‚  â”‚      â”œâ”€day1_measurement3_scene_camera_frame_07798-08274
    |  |      ...
    â”‚  â””â”€near-end
    â”‚      â”œâ”€camera1
    â”‚      â”‚  â”œâ”€day1_measurement1_scene_camera_frame_00523-01000
    â”‚      â”‚  â”œâ”€day1_measurement1_scene_camera_frame_02262-02493
    â”‚      â”‚  â”œâ”€day1_measurement1_scene_camera_frame_06815-07138
    |      |  ...
    â”‚      â”œâ”€camera3
    â”‚      â”‚  â”œâ”€day1_measurement1_scene_camera_frame_00523-01000
    â”‚      â”‚  â”œâ”€day1_measurement1_scene_camera_frame_06815-07138
    â”‚      â”‚  â”œâ”€day1_measurement1_scene_camera_frame_02262-02493
    |      |  ...
    â”‚      â”œâ”€lidar
    â”‚      â”‚  â”œâ”€day1_measurement1_scene_camera_frame_00523-01000
    â”‚      â”‚  â”œâ”€day1_measurement1_scene_camera_frame_02262-02493
    â”‚      â”‚  â”œâ”€day1_measurement1_scene_camera_frame_06815-07138
    |      |  ...
    â”‚      â””â”€camera2
    â”‚          â”œâ”€day1_measurement1_scene_camera_frame_11757-12355
    â”‚          â”œâ”€day1_measurement1_scene_camera_frame_06815-07138
    â”‚          â”œâ”€day1_measurement1_scene_camera_frame_00523-01000
    |          ...
    â”œâ”€gt_json
    â”‚  â”œâ”€near_end
    â”‚  â””â”€far_end
    â”œâ”€pointcloud
    â””â”€lidar_extrinsics
```

<!-- â€»TODO: å¿…è¦ã§ã‚ã‚Œã°ã€å®Ÿéš›ã® `tree` å‡ºåŠ›ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚-->
---

## LiDAR-based Tracking

<!-- â€»OPTIONAL: LiDARã®ã¿ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµæžœã‚’ç¤ºã™å‹•ç”»ï¼ˆä¾‹: `assets/visualizations/lidar_tracking_demo.mp4`ï¼‰ã‚„GIFã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚-->
![tracking preview](visualizations/lidar_only.mp4)

This section describes the LiDAR-based tracking pipeline included in the BasketLiDAR dataset.  
It consists of two major steps, both implemented as Jupyter notebooks.

### 1. From raw LiDAR data to BEV detections

The notebook **`pointcloud_to_BEV-detection.ipynb`** demonstrates how to process raw `.lvx2` point cloud files into a birdâ€™s-eye-view (BEV) representation and generate object detection labels.  
This includes:
- Reading `.lvx2` point cloud data  
- Converting point clouds into BEV maps  
- Applying detection models to produce labeled bounding boxes  

If you need more information about the `.lvx2` file structure, please refer to the **official LVX2 format specification** (https://terra-1-g.djicdn.com/65c028cd298f4669a7f0e40e50ba1131/LVX2%20Specifications.pdf).  

### 2. From BEV detections to tracking results

The notebook **`BEV-detection_to_track.ipynb`** takes the detection labels generated in the previous step as input and produces multi-object tracking results.  
This step links detections across frames to maintain object identities over time.  
A detailed step-by-step manual for the tracking configuration and evaluation will be added in future updates.

---

## Cameraâ€“Fusion Tracking

<!-- â€»OPTIONAL: èžåˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµæžœã®å‹•ç”»ã‚„GIFã‚’ `assets/visualizations/cam_fusion_demo.mp4` ã¨ã—ã¦è¿½åŠ ã—ã¦ãã ã•ã„ã€‚-->

The notebook **`camfusion_postprocess.ipynb`** demonstrates how to perform tracking-by-fusion using both LiDAR and camera information.  
This post-processing step refines LiDAR-based tracks by incorporating visual cues from synchronized RGB frames, improving object continuity and identity preservation.  
As with the LiDAR-based pipeline, a more detailed explanation and reproducibility guide will be released progressively.


## License
 <!-- â€»TODO: æ­£å¼ãªãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’æ˜Žè¨˜ï¼ˆä¾‹: Code: MIT / Data: Custom Research Licenseï¼‰-->

### Code License

All source code in this repository is released under the **MIT License**.  
See the [LICENSE](./LICENSE) file for details.

### Dataset License

The dataset (including LiDAR point clouds, camera images, and annotations)  
is released **for research and educational purposes only** under the following terms:

- Redistribution, rehosting, or public sharing of the dataset, in whole or in part, is **strictly prohibited** without explicit permission from the authors.  
- You may use the dataset internally for **non-commercial research** and **academic publications**.  
- When publishing results based on this dataset, please **cite the corresponding paper** once it becomes available.

> **Summary:**  
> - âœ… Code â†’ MIT License  
> - ðŸš« Data â†’ Research use only / Redistribution prohibited

---

## Contact
Questions, requests, or collaborations are welcome.

- Overview and Dataset request: https://sites.google.com/keio.jp/keio-csg/projects/basket-lidar
- Email: **hayashi.ryu430@keio.jp**  
- Issues: Please open a GitHub issue with a clear title and description.
